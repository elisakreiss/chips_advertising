---
title: "Analysis of German Potato Chips Advertising Texts"
output:
  html_document:
    theme: paper
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction
- establish connection between socio-economic background and food advertisement
- establish connection between low socio-economic class and lower education
- establish connection between low socio-economic class and health


## General remarks about the data

I have collected 25 different chips bags from seven stores (Combi, Norma, Aldi, Lidl, Kaufland, SuperBioMarkt, AllFrisch). I considered each bag which contained "normal" potato chips, i.e., no riffled or in any other way themed type. From those different kinds, I chose (what I considered as) the most basic flavor. Preferably, this was salt (Chipsfrisch, Crunchips, CrustiCroc, DeRit, DorfChips, HandCookedChips, KettleChips, Küstengold, Trafo), but, if this wasn't available, bell pepper (Chio, FeurichChips, JA, JedenTagChips), salt and pepper (FeurichGourmet, KrosseKerle, Naturals, Tyrrells, WorldOfChips), salt and rosmarin (Lisas), salt and vinegar (ClarkysKesselChips, Classic), hot and spicy (Lays) or chives and sour cream (HofChips). The weight of the chips bags ranges from 110g to 200g. In contrast to the original study by Freedman and Jurafsky, I considered the front and the back side of the chips bags, since the advertisement on the front was often very meaningful and non-neglectable. <br>
The text in the data includes all text on the bags (including repetitions and brand names) except for instructions how to store them, the obligatory list of ingredients, the table with nutritional values, the recommended portion size, the store brand (e.g., what Edeka would be for Gut & Günstig potato chips, or Rewe for ja! potato chips), contact information (e.g., address and phone number of the company), complaint information, surveys, recommendations for other products and trade-mark information. However, it is worth mentioning, that if the list of ingredients included a reference to biological farming or that the product is vegan or similar, this information was included. <br>
<!-- There are two different data sets. One that does not and one that does include translations. The default data set does not contain any translations, since translations are not addressing one audience but several. <br> -->
<!-- This leads to problems in word count and categorizations which will be addressed in the respective  -->

```{r LoadingResources, include=FALSE}
library(tidyverse)
library(gridExtra)
library(ngramr)
library(tm)
library(stopwords)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(here)

source(here("analysis","helper_scripts","helpers.r"))
source(here("analysis","helper_scripts","categories.r"))
source(here("analysis","helper_scripts","my_theme.r"))

theme_set(theme_chips(18))
```

## General data considerations

**Language**: There are three potato chips bags (KettleChips, Tyrrells, HandCookedChips) that don't contain German advertisement (or only in the list of ingredients). On the one hand, it can be argued, that those (mostly imports from the UK) do not portray a typically German advertisement strategy. On the other hand, they still compete with mostly German products. <br>
<!-- Therefore, those chips bags will be included in the semantic analysis, but not in the analyses where length of sentences, length of words and word frequency is evaluated, since they are very language specific and could therefore skew the analysis. -->
The other problem to face are the amount of translations. Some potato chips bags don't contain any translations at all (e.g., Lays, JedenTagChips), whereas others (e.g., DeRit) have various translations of their full advertisements resulting in less space for original advertisement. Since translations are so far not included in the transcribed texts, the German total word count doesn't reflect the overall total word count. <br>

**Weight**: The weights of the bags range from 110g to 200g. In my opinion, it is reasonable to assume that the difference between 200g and 175g is neglectable, since the package size does not change a lot. This might still be true for 150g, but from that point on, the packaging becomes perceivably smaller. This means that the amount of space for text decreases and could therefore highly influence an analysis considering total instead of relative amounts. However, when we look at the data, we do not find a positive correlation between increased weight and total word count (rather on the contrary). Therefore, the package size does not seem to influence the total number of words for advertisement in a way that would justify excluding them from an analysis. *it could have an influence though on the price focus... when there is no noticible difference in size, they might compare total prices but as soon as it's smaller, relative prices*

```{r WeightVsWordcount, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
ggplot(df,aes(x=Weight,y=TotalWordCount)) +
  geom_point() +
  geom_smooth(method = "lm")

ggplot(df,aes(x=Price,y=Weight)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Choosing a **price threshold**: 
*I can't find any research to this at all - what do people rather focus on when choosing a product: price (total price) or price per weight (relative price)? Can we do a quick MTurk study or short questionnaire session here?*
<!-- The first decision to make is, whether to consider the price per 100g or the total price. I decided for the total price, because I believe that this is in the end what people base their decision on. However, it should be noted, that this decision only changes the categorization of one item. <br> <br> -->
<!-- A tiny anecdote <br> -->
<!-- *I spent quite some time in the last weeks in supermarkets, taking pictures of all the available potato chips bags. While I was sitting in front of one new shelf, a lady stood next to me and said after some time: "It's always so difficult to decide, right?" She held two potato chips bags in her hands, one being 20 cents cheaper than the other. By that time, I was already pretty familiar with the different kinds of potato chips available and pointed out that she should consider that not only is one of them 20 cents cheaper, but it additionally had 25g less of content. She looked it up, thanked me and said that she did not consider that before.* <br> <br> -->
<!-- This is a great example to emphasize that the price per 100g is not the first thing people look at and personally it showed me, that I spent too much time with potato chips bags. <br> -->

```{r LoadingData_BasicAnalysis, message=FALSE, warning=FALSE, include=FALSE}
basic_info = read_csv(here("texts","chips_data.csv"))

df = basic_info

# think about Funny Chipsfrisch and Trafo, since they were on discount price

# mark products that mainly have English advertisement
df$EnglishOnly = ifelse((df$Product_name=="KettleChips" | df$Product_name=="Tyrrells" | df$Product_name=="HandCookedChips"), TRUE, FALSE)

# for easier subsequent analysis, delete special characters from texts
df$Text_edited = df$Text %>%
  str_replace_all("\\,|\\?|\\!|\\.","") %>%
  str_to_lower()

# splitting the data into "expensive" and "cheap" potato chips bags
meanPrice = sum(df$Price)/length(df$Price)
df$PriceCategory = ifelse(df$Price >= meanPrice,"expensive","cheap")

df$TotalWordCount = str_count(df$Text_edited, boundary("word"))
```

## Connection educational level and socioeconomic status

Education is one of the main indicators of socioeconomic status, together with income and occupation (cite: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1694190/, https://www.nap.edu/read/11036/chapter/6). Therefore, if the advertisement on potato chips bags reflects socioeconomic groups, we could expect that this is reflected in the language. Freedman and Jurafsky (2011) investigate this question by looking at the **total word count** per chips bag, **sentence complexity** and **word frequency**.

### Total word count and the problem of translations

A first intuitive measure to compare the language of advertisement on different potato chips bags is to count the total number of words used. The idea is that an increased number of words appeals more to an audience with a higher educational level. Indeed, Freedman and Jurafsky (2011) could show the tendency that the average word count increases with increasing price (104 words on average for inexpensive chips vs. 142 for expensive ones, resulting in a difference of 38 words).<br>
Before collecting data, I assumed that this can be expected from German potato chips advertisement as well, since I could not foresee any difference in culture in that respect. In fact, we do see a tendency for more words on more expensive potato chips bags, but the difference was lower than I expected (47 words on average for inexpensive chips vs. 77 for expensive ones, resulting in a difference of 30 words).<br>
When collecting the data for this study, I noticed one problem: What should I do with translations? Translations did not seem to be a problem when dealing with US-American products, but they turn out to be highly important when considering German potato chips advertisement. <br>
For one, the amount of translations varies a lot between potato chips bags. Some bags do not have any translations and others have whole advertisement paragraphs phrased in up to four and ingredients in up to eleven languages. The amount of translations on a bag therefore highly influences how much space is left for additional advertisement. <br>
However, this would not be important in this case, if the amount of translations was independent of the price. If the amount of translated words was evenly distributed over the whole price range, it would not have an effect on the difference of the total word count between cheap and expensive potato chips. But this is not what we see. We see that more expensive potato chips are a lot more likely to have translated advertisement than cheap ones, which is therefore relevant to consider when evaluating total word count. <br>
When we now consider all words used for advertisement (original language and translations) and look at the difference between total word count again, we see that overall text increases remarkably with rising prices (50 words on average for inexpensive chips vs. 141 for expensive ones, resulting in a difference of 91 words).

```{r Plot_WordCountTransl, echo=FALSE, fig.height=12, fig.width=13, message=FALSE, warning=FALSE}

df_withTranslation = read_csv(here("texts","chips_data_withTranslations.csv"))
df_withTranslation$Text = gsub(",|?|!|\\.","",df_withTranslation$Text) %>%
  str_to_lower()
df_withTranslation$PriceCategory = ifelse(df_withTranslation$Price >= 1.5,"expensive","cheap")
df_withTranslation$TotalWordCount = str_count(df_withTranslation$Text, boundary("word"))
df_withTranslation$TranslatedWords = df_withTranslation$TotalWordCount - df$TotalWordCount

toplot = df %>%
  add_column(TL=FALSE)
toplotTL = df_withTranslation %>%
  add_column(TL=TRUE)

# Scatter plot
toplot_all = bind_rows(toplot,toplotTL)

scatterPlot = ggplot(toplot_all,aes(x=Price,y=TotalWordCount,color=TL)) +
  geom_point() +
  ylab("Number of Words") +
  theme(legend.position="top") +
  scale_color_manual(name ="Translations",
                       labels=c("Without", "With"),
                       values = c("#6fb76f","#b76fb7")) +
  geom_smooth(method = "lm") +
  # geom_text(aes(label=Product_name),hjust=1, vjust=-0.5) +
  xlim(0.5,2.5)

# Bar plot
plot_totalWordCountTL = toplotTL %>%
  select(PriceCategory,TotalWordCount) %>%
  group_by(PriceCategory) %>%
  summarise(AvgTotalWordCount = mean(TotalWordCount), ci.low=ci.low(TotalWordCount), ci.high=ci.high(TotalWordCount)) %>%
  add_column(TL=TRUE) %>%
  add_column(YMin=.$AvgTotalWordCount-.$ci.low)  %>%
  add_column(YMax=.$AvgTotalWordCount+.$ci.high)

plot_totalWordCount = toplot %>%
  select(PriceCategory,TotalWordCount) %>%
  group_by(PriceCategory) %>%
  summarise(AvgTotalWordCount = mean(TotalWordCount), ci.low=ci.low(TotalWordCount), ci.high=ci.high(TotalWordCount)) %>%
  add_column(TL=FALSE) %>%
  add_column(YMin=.$AvgTotalWordCount-.$ci.low)  %>%
  add_column(YMax=.$AvgTotalWordCount+.$ci.high)

plot_totalWordCount_all = bind_rows(plot_totalWordCount,plot_totalWordCountTL)

binaryPlot = ggplot(plot_totalWordCount_all,aes(x=PriceCategory,y=AvgTotalWordCount,fill=TL)) +
  geom_col(position = position_dodge(), width = .5) +
  xlab("Price") +
  ylab("Number of Words") +
  theme(legend.position="top") +
  scale_fill_manual(name="Translations",
                    values = c("#6fb76f","#b76fb7"),
                    labels=c("Without", "With")) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

# plot translations only
plot_translatedWordCount = toplotTL %>%
  select(PriceCategory,TranslatedWords) %>%
  group_by(PriceCategory) %>%
  summarise(AvgTranslatedWords = mean(TranslatedWords), ci.low=ci.low(TranslatedWords), ci.high=ci.high(TranslatedWords)) %>%
  add_column(TL=TRUE) %>%
  add_column(YMin=.$AvgTranslatedWords-.$ci.low)  %>%
  add_column(YMax=.$AvgTranslatedWords+.$ci.high)

translation_plot_bar = ggplot(plot_translatedWordCount,aes(x=PriceCategory,y=AvgTranslatedWords,fill=TL)) +
  geom_col(position = position_dodge(), width = .5) +
  xlab("Price") +
  ylab("Number of Words") +
  theme(legend.position="top") +
  scale_fill_manual(name="Translations",
                    values = c("#6fb76f","#b76fb7"),
                    labels=c("Without", "With")) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

translation_plot_scatter = ggplot(toplotTL,aes(x=Price,y=TranslatedWords)) +
  geom_point() +
  geom_smooth(method = "lm", color="#6fb76f") +
  xlab("Price") +
  ylab("Number of Translated Words")

grid.arrange(scatterPlot,binaryPlot,translation_plot_scatter,translation_plot_bar,ncol=2)
```

### Sentence Complexity
##### Average word length
In a lot of languages, it is reasonable to assume that longer words address a more educated audience than shorter ones. Freedman and Jurafsky (2011) found this effect in potato chips advertisement as well. More expensive potato chips had an increased average word length compared to cheap ones. In German, we don't find this at all. <br>
Why could that be the case? German is a language that lives of compunding. The English "bell pepper flavour" becomes "Paprikageschmack", which is not more difficult to comprehend or in general less frequently used than non-compunded structures. In German, we rather find a contrary movement, which is that splitting up a compound in a more complex syntactical structure is more considered a sign of intellect than using those long compounds. (*elaborate here, cite someone*)

```{r Plot_AvgWordLength, echo=FALSE, fig.width = 13, fig.height = 5}

toplot = df %>%
  # take only chips bags with mainly German advertisement
  filter(EnglishOnly == FALSE) %>%
  # exclude stopwords
  add_column(Text_nostopwords=removeWords(.$Text_edited,stopwords::stopwords("de", source = "snowball"))) %>%
  # add total word count and average word count
  add_column(TotalWordCount_nostopwords=str_count(.$Text_edited,boundary("word"))) %>%
  add_column(AverageWordLength_nostopwords = str_replace_all(.$Text_edited," |-","") %>%
                       nchar(type="chars")/.$TotalWordCount) %>%
  
  select(PriceCategory,AverageWordLength_nostopwords) %>%
  group_by(PriceCategory) %>%
  summarise(AvgAverageWordLength_nostopwords = mean(AverageWordLength_nostopwords), ci.low=ci.low(AverageWordLength_nostopwords), ci.high=ci.high(AverageWordLength_nostopwords)) %>%
  add_column(YMin=.$AvgAverageWordLength_nostopwords-.$ci.low)  %>%
  add_column(YMax=.$AvgAverageWordLength_nostopwords+.$ci.high)

binaryPlot = ggplot(toplot,aes(x=PriceCategory,y=AvgAverageWordLength_nostopwords)) +
  geom_col(fill="#4D8069", width=.5) +
  xlab("Price") +
  ylab("Average Word Length") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

grid.arrange(binaryPlot,ncol=2)
```

##### Sentence Length
Thoughts: <br>
- hypothesis (from Freedman & Jurafsky): "more expensive chips would be packaged in more expensive language" <br>
- their chosen measure: Flesch-Kincaid readability test = weighted value of the length of each sentence in the text (longer sentences tend to be more complex) and the length of each word in the text in syllables (longer words tend to be harder to read) *can I get Coleman-Liau readability test results? Do I need them* <br>
- since syllable extraction is rather difficult (in German), look at Coleman-Liau readability test results <br>
- CLR results for expensive and inexpensive chips are very similar <br>
- looking at components: CLR_L (letters per 100 words) is basically identical; CLR_S (sentences per 100 words) shows clear distinction <br>
- we have already analyzed above why letters per word is a difficult measure in German since compound nouns are not necessarily that much more difficult to comprehend (*cite*) (this would also be a problem for the syllables case); looking at the values, CLR_L is generally much bigger than CLR_S which is why the higher "weight" causes the final distribution to look more like CLR_L <br>
- therefore this analysis is not really informative <br>
- maybe look at naive approach then: only considering sentence length (number of words/number of sentences) for each bag <br>
- what we see: slight tendency for longer sentences, but nothing remarkable <br>

```{r Plot_AvgSentenceLength, echo=FALSE, fig.height=5, fig.width=13, message=FALSE, warning=FALSE}

toplot = df %>%
  # take only chips bags with mainly German advertisement
  filter(EnglishOnly == FALSE) %>% 
  # all sentences are separated by .
  mutate(Text_dot = str_replace_all(Text,"!",".")) %>% 
  # average sentence length = number of words/number of sentences
  mutate(AvgSentenceLength = TotalWordCount/str_count(Text_dot,"\\."))
  # mutate(AvgSentenceLength = str_length(Text_dot)/str_count(Text_dot,"\\."))
  
plot_avgSentence_Words = toplot %>%
  select(PriceCategory,AvgSentenceLength) %>%
  group_by(PriceCategory) %>%
  summarise(BinnedAvgSentenceLength = mean(AvgSentenceLength), ci.low=ci.low(AvgSentenceLength), ci.high=ci.high(AvgSentenceLength)) %>%
  add_column(YMin=.$BinnedAvgSentenceLength-.$ci.low)  %>%
  add_column(YMax=.$BinnedAvgSentenceLength+.$ci.high)

binaryPlot_words = ggplot(plot_avgSentence_Words,aes(x=PriceCategory,y=BinnedAvgSentenceLength)) +
  geom_col(fill="#4D8069", width=.5) +
  xlab("Price") +
  ylab("Average Sentence Length") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

# scatterplot

# binaryPlot_words = ggplot(toplot,aes(x=Price,y=AvgSentenceLength)) +
#   geom_point(fill="#4D8069", width=.5) +
#   geom_smooth(method = "lm") +
#   xlab("Price") +
#   ylab("Average Sentence Length")

grid.arrange(binaryPlot_words,ncol=2)
```

```{r CLR, eval=FALSE, include=FALSE}
###
# Coleman-Liau readability test
###

# L = average number of letters per 100 words and;
# S = average number of sentences per 100 words.
# 0.0588L – 0.296S – 15.8

df_clr = toplot %>% 
  mutate(CLR_L=(100*str_length(Text_dot))/(str_count(Text_dot,boundary("word")))) %>% 
  mutate(CLR_S=(100*str_count(Text_dot,"\\."))/(str_count(Text_dot,boundary("word")))) %>% 
  mutate(CLR=(0.0588*CLR_L-0.296*CLR_S-15.8))

plot_CLR = df_clr %>%
  select(PriceCategory,CLR) %>%
  group_by(PriceCategory) %>%
  summarise(AvgCLR = mean(CLR), ci.low=ci.low(CLR), ci.high=ci.high(CLR)) %>%
  add_column(YMin=.$AvgCLR-.$ci.low)  %>%
  add_column(YMax=.$AvgCLR+.$ci.high)

binaryPlot_CLR = ggplot(plot_CLR,aes(x=PriceCategory,y=AvgCLR)) +
  geom_col(fill="#4D8069", width=.5) +
  xlab("Price") +
  ylab("Average Sentence Length (CLR)") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

plot_CLR_L = df_clr %>%
  select(PriceCategory,CLR_L) %>%
  group_by(PriceCategory) %>%
  summarise(AvgCLR_L = mean(CLR_L), ci.low=ci.low(CLR_L), ci.high=ci.high(CLR_L)) %>%
  add_column(YMin=.$AvgCLR_L-.$ci.low)  %>%
  add_column(YMax=.$AvgCLR_L+.$ci.high)

binaryPlot_CLR_L = ggplot(plot_CLR_L,aes(x=PriceCategory,y=AvgCLR_L)) +
  geom_col(fill="#4D8069", width=.5) +
  xlab("Price") +
  ylab("Average Number of Letters per 100 Words (CLR_L)") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

plot_CLR_S = df_clr %>%
  select(PriceCategory,CLR_S) %>%
  group_by(PriceCategory) %>%
  summarise(AvgCLR_S = mean(CLR_S), ci.low=ci.low(CLR_S), ci.high=ci.high(CLR_S)) %>%
  add_column(YMin=.$AvgCLR_S-.$ci.low)  %>%
  add_column(YMax=.$AvgCLR_S+.$ci.high)

binaryPlot_CLR_S = ggplot(plot_CLR_S,aes(x=PriceCategory,y=AvgCLR_S)) +
  geom_col(fill="#4D8069", width=.5) +
  xlab("Price") +
  ylab("Average Number of Sentences per 100 Words (CLR_S)") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

grid.arrange(binaryPlot_words,binaryPlot_CLR,binaryPlot_CLR_L,binaryPlot_CLR_S,ncol=2)
```


### Word Frequency
Kartoffel: frequent <br>
Chips: frequent <br>
Kartoffelchips: infrequent <br>
<br>
potato: frequent <br>
chips: frequent <br>
potato chips: frequent + frequent <br>

```{r Plot_AvgWordFrequency, echo=FALSE, fig.height=5, fig.width=13, message=FALSE, warning=FALSE}

# average word frequency
freq_noStopwords = read_csv(here("analysis","frequencies","no_stopwords_freq.csv"))

toplot = df %>%
  left_join(freq_noStopwords,by=c("Product_name"="Product_ID")) %>%
  rename(AvgWordFreq=AvgFreq) %>%
  filter(EnglishOnly == FALSE) %>%
  select(PriceCategory,AvgWordFreq) %>%
  group_by(PriceCategory) %>%
  summarise(AvgAvgWordFreq = mean(AvgWordFreq), ci.low=ci.low(AvgWordFreq), ci.high=ci.high(AvgWordFreq)) %>%
  add_column(YMin=.$AvgAvgWordFreq-.$ci.low)  %>%
  add_column(YMax=.$AvgAvgWordFreq+.$ci.high)

binaryPlot = ggplot(toplot,aes(x=PriceCategory,y=AvgAvgWordFreq)) +
  geom_col(fill="#4D8069", width=.5) +
  xlab("Price") +
  ylab("Average Word Freq") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

grid.arrange(binaryPlot,ncol=2)
```

## Semantic Analysis (find better title)

### Categorization

Special phrases used in the advertisement are sorted into the following main categories: Health Phrases, Distinctive Phrases, Preparation Phrases, Authenticity Phrases and LicensePhrases. This categorization is strongly inspired by Freedman and Jurafsky (2011). <br>
Generally, phrases are attributed to the one group where the connection is most salient. There are a few that are categorized in more than one group (19 out of 173), but they are the exception.

#### Health Phrases
The first association when reading a health phrase should be, that it is good for you and your body. This category is strongly connected to Naturalness Phrases, because phrases that appeal to us as natural and non-artificial are often strongly associated with being healthy (e.g., "bio" (organic)).<br>
Original (and translated) examples for this category are "ohne Konservierungsstoffe" (no preservatives), "glutenfrei" (gluten free), "vegetarier geeignet" (suitable for vegetarians) and "reich an ungesättigten fettsäuren" (rich in unsaturated fatty acids). <br>

#### Distinctive Phrases
Expressions that belong into this category express some differentiating quality against other products. <br>
**Comparative Phrases** consist of superlatives (e.g., "beste" (best)) and other expressions that define the product as outstunding (e.g., "beste" (best), "einzigartig" (unique), "außergewöhnlich" (extraordinary)) <br>
**Negative Markers** contain all negations, such as "ohne" (no/without) and "-frei" (-free). <br>

#### Authenticity Phrases
Phrases sorted into this category make the reader build a connection to the product. <br>
**Naturalness Phrases** emphasize the pureness and simplicity of the product (e.g., "bio" (organic), "von Natur aus" (inherently), "ohne Konservierungsstoffe" (no preservatives)). <br>
**Historicity Phrases** communicate the long tradition of the company and the product (e.g., "in 3. generation" (in third generation)). <br>
**Location Phrases** draw references to places one probably already has a connection to (e.g., "aus besten deutschen Anbauregionen" (from the best German cultivations), but also "für die Menschen von hier" (for people from here), and simply "aus der Heimat" (from home)). <br>
**Genuity Phrases** aim at making (mainly) the company approachable (e.g. by emphasizing that it's a family business through words such as "Familienbetrieb" (family business) or "familieneigen" (family-owned)). Additionally, those phrases count into it that communicate the long tradition of the company (e.g., "in 3. generation" (in third generation)). <br>
**Preparation Phrases**
A rule of thumb for phrases for this category is, that those are the phrases which generally belongs in a recipe. <br>
*Ingredient Phrases* comprise mentions of ingredients (e.g., "Salz" (salt)) or phrases that allow inferences about the ingredients, such as "vegetariergeeignet" (suitable for vegetarians). <br>
*Process Phrases* describe what is done to the ingredients (e.g. mentioning that they are roasted ("geröstet")). <br>
**Quality Phrases**
Comprised in this category are phrases that ensure (or at least promise) quality (e.g., "guaranteed", "controlled", "certified").

```{r Categorization, include=FALSE}

df$HealthPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$HealthPhrase))

df$ComparativePhrases = str_count(df$Text,pattern = cat_pattern(Phrases$ComparativePhrase))
df$NegativeMarkers = str_count(df$Text,pattern = cat_pattern(Phrases$NegativeMarker))

df$IngredientPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$IngredientPhrase))
df$ProcessPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$ProcessPhrase))

df$NaturalnessPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$NaturalnessPhrase))
df$HistoricityPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$HistoricityPhrase))
df$LocationPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$LocationPhrase))
df$GenuityPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$GenuityPhrase))
df$QualityPhrases = str_count(df$Text,pattern = cat_pattern(Phrases$QualityPhrase))

# summaries
df$DistinctivePhrases = df$ComparativePhrases + df$NegativeMarkers
df$PreparationPhrases = df$IngredientPhrases + df$ProcessPhrases
df$AuthenticityPhrases = df$NaturalnessPhrases + df$HistoricityPhrases + df$LocationPhrases + df$GenuityPhrases + df$QualityPhrases + df$PreparationPhrases
```

### Total and relative use of Authenticity, Distinctive, Health phrases
At the heart of the study lies the analysis of which topics are addressed in these advertisements and whether we can find a regularity of how this changes with the price that those potato chips bags are sold for. <br>
There are two ways of comparing the amount of these phrases of interest in cheap and expensive potato chips bags: a) counting the total amount of phrases per bag, b) taking the amount relative to the total number of words per bag. Which version we decide for depends on the question we want to answer and the perspective we have. When looking at the relative instead of total numbers, we ask for example, how many percent of the text on each potato chips bag consists of authenticity phrases. It turns out that when we look at the total amount (so how many times authenticity phrases occur on a potato chips bag), they are mentioned more often on expensive than on cheap ones. But when we look at the proportion of authenticity phrases on what is written, expensive potato chips score even slightly lower than cheap chips. We can conclude that even though cheap potato chips bags write less about Authenticity than expensive chips, in relation to the amount they write, they concentrate more on this. (*elaborate: are authenticity phrases the only phrases where it makes a difference on who uses more?*)

<!-- Plot Authenticity, Distinctive, Health Phrases (no differences to relative) -->

```{r PhrasePlotGrid, eval=FALSE, fig.height=5, fig.width=15, include=FALSE}

plot_HealthPhrases = df %>%
  select(PriceCategory,HealthPhrases) %>%
  group_by(PriceCategory) %>%
  summarise(AvgHealthPhrases = mean(HealthPhrases), ci.low=ci.low(HealthPhrases), ci.high=ci.high(HealthPhrases)) %>%
  add_column(YMin=.$AvgHealthPhrases-.$ci.low)  %>%
  add_column(YMax=.$AvgHealthPhrases+.$ci.high)

HP_plot = ggplot(plot_HealthPhrases,aes(x=PriceCategory,y=AvgHealthPhrases)) +
  ggtitle("Health") +
  geom_col(fill="#66c2a5", width=.5) +
  xlab("") +
  # ylab("") +
  ylab("Total Number of Phrases") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

plot_DistinctivePhrases = df %>%
  select(PriceCategory,DistinctivePhrases) %>%
  group_by(PriceCategory) %>%
  summarise(AvgDistinctivePhrases = mean(DistinctivePhrases), ci.low=ci.low(DistinctivePhrases), ci.high=ci.high(DistinctivePhrases)) %>%
  add_column(YMin=.$AvgDistinctivePhrases-.$ci.low)  %>%
  add_column(YMax=.$AvgDistinctivePhrases+.$ci.high)

DP_plot = ggplot(plot_DistinctivePhrases,aes(x=PriceCategory,y=AvgDistinctivePhrases)) +
  ggtitle("Distinctive") +
  geom_col(fill="#fc8d62", width=.5) +
  xlab("") +
  ylab("") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

plot_AuthenticityPhrases = df %>%
  select(PriceCategory,AuthenticityPhrases) %>%
  group_by(PriceCategory) %>%
  summarise(AvgAuthenticityPhrases = mean(AuthenticityPhrases), ci.low=ci.low(AuthenticityPhrases), ci.high=ci.high(AuthenticityPhrases)) %>%
  add_column(YMin=.$AvgAuthenticityPhrases-.$ci.low)  %>%
  add_column(YMax=.$AvgAuthenticityPhrases+.$ci.high)

AP_plot = ggplot(plot_AuthenticityPhrases,aes(x=PriceCategory,y=AvgAuthenticityPhrases)) +
  ggtitle("Authenticity") +
  geom_col(fill="#8da0cb", width=.5) +
  xlab("") +
  ylab("") +
  # ylab("Number of Authenticity Phrases") +
  # ylab("Total Number of Phrases") +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="grey")

grid.arrange(HP_plot,DP_plot,AP_plot,ncol=3)
```

```{r Plot_TotalPhrases, echo=FALSE, fig.height=5, fig.width=13, message=FALSE, warning=FALSE}
# no difference to relative
plot_Phrases = df %>%
  gather(HealthPhrases,DistinctivePhrases,AuthenticityPhrases,key = "Phrases",value = "Occurrences") %>%
  select(PriceCategory,Phrases,Occurrences) %>%
  group_by(PriceCategory,Phrases) %>%
  summarise(AvgOccurrences = mean(Occurrences), ci.low=ci.low(Occurrences), ci.high=ci.high(Occurrences)) %>%
  add_column(YMin=.$AvgOccurrences-.$ci.low)  %>%
  add_column(YMax=.$AvgOccurrences+.$ci.high)
plot_Phrases$Phrases = factor(plot_Phrases$Phrases, levels = c("HealthPhrases","DistinctivePhrases","AuthenticityPhrases"))

plot_total = ggplot(plot_Phrases,aes(x=PriceCategory,y=AvgOccurrences,fill=Phrases)) +
  # ggtitle("Total") +
  geom_col(position = position_dodge(), width = .5) +
  xlab("Price") +
  ylab("Total Number of Occurrences") +
  theme(plot.title=element_text(colour = "darkgrey", hjust = 0.5, face = "bold")) +
  theme(legend.position="top") +
  # scale_fill_manual(values = c("#66c2a5","#fc8d62","#8ca0cb","#e789c3","#a6d854"),
  scale_fill_manual(values = c("#66c2a5","#fc8d62","#8da0cb"),
                    labels=c("Health", "Distinctive", "Authenticity")) +
  # guides(fill=guide_legend(nrow=2,byrow=TRUE)) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

grid.arrange(plot_total,ncol=2)
```

Plot Phrases against Price

```{r Plot_PhrasesPrice, echo=FALSE, fig.height=5, fig.width=13, message=FALSE, warning=FALSE}
HeP_P = ggplot(df,aes(x=Price,y=HealthPhrases,color=PriceCategory))+
  geom_point() +
  scale_color_manual(values=c("#84ceb7", "#519b84")) +
  theme(legend.position="none") +
  geom_smooth(method="lm",color="#66c2a5",aes(x=Price,y=HealthPhrases))

DiP_P = ggplot(df,aes(x=Price,y=DistinctivePhrases,color=PriceCategory))+
  geom_point() +
  scale_color_manual(values=c("#fca381", "#c9704e")) +
  theme(legend.position="none") +
  geom_smooth(method="lm",color="#fc8d62",aes(x=Price,y=DistinctivePhrases))

AP_P = ggplot(df,aes(x=Price,y=AuthenticityPhrases,color=PriceCategory))+
  geom_point() +
  scale_color_manual(values=c("#afbcda", "#62708e")) +
  theme(legend.position="none") +
  geom_smooth(method="lm",color="#8da0cb",aes(x=Price,y=AuthenticityPhrases))

grid.arrange(HeP_P,DiP_P,AP_P,ncol=3)
```


Plot **total/relative amount of different DistinctivePhrases** for expensive next to cheap potato chips bags

```{r Plot_Distinctive, echo=FALSE, fig.width = 13, fig.height = 6}
plot_Phrases = df %>%
               gather(ComparativePhrases,NegativeMarkers,key = "Phrases",value = "Occurrences") %>%
               select(PriceCategory,Phrases,Occurrences) %>%
               group_by(PriceCategory,Phrases) %>%
               summarise(AvgOccurrences = mean(Occurrences), ci.low=ci.low(Occurrences), ci.high=ci.high(Occurrences))
plot_Phrases$YMin = plot_Phrases$AvgOccurrences - plot_Phrases$ci.low
plot_Phrases$YMax = plot_Phrases$AvgOccurrences + plot_Phrases$ci.high

plot_total = ggplot(plot_Phrases,aes(x=PriceCategory,y=AvgOccurrences,fill=Phrases)) +
  geom_col(position = position_dodge(), width = .5) +
  theme(legend.position="top") +
  scale_fill_manual(values = c("#fca381","#c9704e"),
                    labels=c("Comparative", "Negative")) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

# relativePhrases = df %>%
#   select(PriceCategory,ComparativePhrases,NegativeMarkers) %>%
#   mutate_at(vars(-PriceCategory),funs(./df$TotalWordCount))
# 
# plot_relPhrases = relativePhrases %>%
#   gather(ComparativePhrases,NegativeMarkers,key = "Phrases",value = "Occurrences") %>%
#   select(PriceCategory,Phrases,Occurrences) %>%
#   group_by(PriceCategory,Phrases) %>%
#   summarise(AvgRelOccurrences = mean(Occurrences), ci.low=ci.low(Occurrences), ci.high=ci.high(Occurrences)) %>%
#   add_column(YMin=.$AvgRelOccurrences-.$ci.low)  %>%
#   add_column(YMax=.$AvgRelOccurrences+.$ci.high)
# 
# plot_relative = ggplot(plot_relPhrases,aes(x=PriceCategory,y=AvgRelOccurrences,fill=Phrases)) +
#   geom_col(position = position_dodge(), width = .5) +
#   theme(legend.position="top") +
#   scale_fill_manual(values = c("#eeacd5","#a15f88"),
#                     labels=c("Comparative", "Negative")) +
#   geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

grid.arrange(plot_total,ncol=2)
```

Plot **total/relative amount of different AuthenticityPhrases** for expensive next to cheap potato chips bags
Causing difference in Authenticity between total and relative count: mainly ingredient phrases!

```{r Plot_Authenticity, echo=FALSE, fig.width = 13, fig.height = 8}
# relative do not make much of a difference
plot_Phrases = df %>%
  gather(NaturalnessPhrases,LocationPhrases,GenuityPhrases,HistoricityPhrases,QualityPhrases,PreparationPhrases,key = "Phrases",value = "Occurrences") %>%
  select(PriceCategory,Phrases,Occurrences) %>%
  group_by(PriceCategory,Phrases) %>%
  summarise(AvgOccurrences = mean(Occurrences), ci.low=ci.low(Occurrences), ci.high=ci.high(Occurrences)) %>%
  add_column(YMin=.$AvgOccurrences-.$ci.low)  %>%
  add_column(YMax=.$AvgOccurrences+.$ci.high)

plot_total = ggplot(plot_Phrases,aes(x=PriceCategory,y=AvgOccurrences,fill=Phrases)) +
  geom_col(position = position_dodge(), width = .5) +
  theme(legend.position="top") +
  xlab("Price") +
  ylab("Total Number of Phrases") +
  scale_fill_manual(values = c("#d1d9ea","#afbcda","#8da0cb", "#7080a2", "#465065", "#2a303c"),
                    labels=c("Genuity", "Historicity", "Location", "Naturalness", "Preparation", "Quality")) +
  guides(fill=guide_legend(nrow=1,byrow=FALSE)) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))


grid.arrange(plot_total,ncol=1)
```

Plot **total/relative amount of different PreparationPhrases** for expensive next to cheap potato chips bags

```{r Plot_Preparation, echo=FALSE, fig.width = 13, fig.height = 6}
plot_Phrases = df %>%
               gather(IngredientPhrases,ProcessPhrases,key = "Phrases",value = "Occurrences") %>%
               select(PriceCategory,Phrases,Occurrences) %>%
               group_by(PriceCategory,Phrases) %>%
               summarise(AvgOccurrences = mean(Occurrences), ci.low=ci.low(Occurrences), ci.high=ci.high(Occurrences))
plot_Phrases$YMin = plot_Phrases$AvgOccurrences - plot_Phrases$ci.low
plot_Phrases$YMax = plot_Phrases$AvgOccurrences + plot_Phrases$ci.high

plot_total = ggplot(plot_Phrases,aes(x=PriceCategory,y=AvgOccurrences,fill=Phrases)) +
  geom_col(position = position_dodge(), width = .5) +
  theme(legend.position="top") +
  scale_fill_manual(values = c("#7d8493","#313846"),
                    labels=c("Ingredient", "Process")) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

relativePhrases = df %>%
  select(PriceCategory,IngredientPhrases,ProcessPhrases) %>%
  mutate_at(vars(-PriceCategory),funs(./df$TotalWordCount))

plot_relPhrases = relativePhrases %>%
  gather(IngredientPhrases,ProcessPhrases,key = "Phrases",value = "Occurrences") %>%
  select(PriceCategory,Phrases,Occurrences) %>%
  group_by(PriceCategory,Phrases) %>%
  summarise(AvgRelOccurrences = mean(Occurrences), ci.low=ci.low(Occurrences), ci.high=ci.high(Occurrences)) %>%
  add_column(YMin=.$AvgRelOccurrences-.$ci.low)  %>%
  add_column(YMax=.$AvgRelOccurrences+.$ci.high)

plot_relative = ggplot(plot_relPhrases,aes(x=PriceCategory,y=AvgRelOccurrences,fill=Phrases)) +
  geom_col(position = position_dodge(), width = .5) +
  theme(legend.position="top") +
  scale_fill_manual(values = c("#7d8493","#313846"),
                    labels=c("Ingredient", "Process")) +
  geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.2,color="grey",position = position_dodge(width=.5))

grid.arrange(plot_total,plot_relative,ncol=2)
# grid.arrange(plot_relative,ncol=2)
```


```{r WordClouds, eval=FALSE, include=FALSE}
cheap_text = paste(df[df$PriceCategory=="cheap",]$Text,collapse="")  
exp_text = paste(df[df$PriceCategory=="expensive",]$Text,collapse="") 

docs_cheap <- Corpus(VectorSource(cheap_text))
docs_exp <- Corpus(VectorSource(exp_text))

# inspect(docs_cheap)

# Remove german/english common stopwords
docs_cheap <- tm_map(docs_cheap, removeWords, stopwords("english"))
docs_cheap <- tm_map(docs_cheap, removeWords, stopwords("german"))
docs_cheap <- tm_map(docs_cheap, removeWords, c("chips","kartoffelchips","kesselchips","kartoffeln"))
docs_exp <- tm_map(docs_exp, removeWords, stopwords("english"))
docs_exp <- tm_map(docs_exp, removeWords, stopwords("german"))
docs_exp <- tm_map(docs_exp, removeWords, c("chips","kartoffelchips","kesselchips","kartoffeln"))
docs_cheap <- tm_map(docs_cheap, stripWhitespace)
docs_exp <- tm_map(docs_exp, stripWhitespace)

dtm_cheap <- TermDocumentMatrix(docs_cheap)
m_cheap <- as.matrix(dtm_cheap)
v_cheap <- sort(rowSums(m_cheap),decreasing=TRUE)
d_cheap <- data.frame(word = names(v_cheap),freq=v_cheap)
# head(d_cheap, 10)

dtm_exp <- TermDocumentMatrix(docs_exp)
m_exp <- as.matrix(dtm_exp)
v_exp <- sort(rowSums(m_exp),decreasing=TRUE)
d_exp <- data.frame(word = names(v_exp),freq=v_exp)
# head(d_exp, 10)

set.seed(1234)
wordcloud(words = d_exp$word, freq = d_exp$freq, min.freq = 3, scale=c(2,.2),
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))

set.seed(1234)
wordcloud(words = d_cheap$word, freq = d_cheap$freq, min.freq = 3, scale=c(2,.2),
          max.words=200, random.order=FALSE, rot.per=0.35, 
          colors=brewer.pal(8, "Dark2"))
```

```{r}

# df_withTranslation$TranslatedWords_centered = df_withTranslation$TranslatedWords - mean(df_withTranslation$TranslatedWords)
# df_withTranslation$Price_centered = df_withTranslation$Price - mean(df_withTranslation$Price)
# df_withTranslation$TotalWordCount_centered = df_withTranslation$TotalWordCount - mean(df_withTranslation$TotalWordCount)
# 
# cor(df_withTranslation$TranslatedWords, df_withTranslation$Price)
# model = lm(TranslatedWords_centered ~ Price_centered, data=df_withTranslation)
# # model = lm(Price_centered ~ TranslatedWords_centered, data=df_withTranslation)
# summary(model)
# 
# cor(df_withTranslation$Price, df_withTranslation$TotalWordCount)
# model = lm(TotalWordCount ~ Price, data=df_withTranslation)
# summary(model)
```

```{r Plot_TranslatedWords, echo=FALSE, fig.width = 13, fig.height = 5}

# plot_TranslatedWords = df_withTranslation %>%
#   select(PriceCategory,TranslatedWords) %>%
#   group_by(PriceCategory) %>%
#   summarise(AvgTranslatedWords = mean(TranslatedWords), ci.low=ci.low(TranslatedWords), ci.high=ci.high(TranslatedWords)) %>%
#   add_column(YMin=.$AvgTranslatedWords-.$ci.low)  %>%
#   add_column(YMax=.$AvgTranslatedWords+.$ci.high)
# 
# binaryPlot = ggplot(plot_TranslatedWords,aes(x=PriceCategory,y=AvgTranslatedWords)) +
#   geom_col(fill="#694d80", width=.5) +
#   xlab("Price") +
#   ylab("Number of Translated Words") +
#   geom_errorbar(aes(ymin=YMin,ymax=YMax),width=.25,color="darkgrey")
# 
# binaryPlot1 = ggplot(df_withTranslation,aes(x=TranslatedWords_centered,y=Price_centered)) +
#   geom_point() +
#   geom_smooth(method="lm")
# 
# binaryPlot2 = ggplot(df_withTranslation,aes(x=Price_centered,y=TranslatedWords_centered)) +
#   geom_point() +
#   geom_smooth(method="lm")
# 
# 
# grid.arrange(binaryPlot1,binaryPlot2,ncol=2)
```








